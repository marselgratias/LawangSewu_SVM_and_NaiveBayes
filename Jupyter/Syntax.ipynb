{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Tentukan path ke file Excel\n",
    "file_path = r'D:\\SAMUEL\\data mentah.xlsx'\n",
    "\n",
    "# Gunakan pandas untuk membaca file Excel\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Tampilkan isi DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISIS DESKRIPTIF RATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melihat banyak ulasan dari tiap rating\n",
    "\n",
    "df[\"Rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Menghitung jumlah ulasan untuk setiap rating\n",
    "rating_counts = df['Rating'].value_counts().sort_index()\n",
    "\n",
    "# Daftar warna yang berbeda untuk setiap bar\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(rating_counts)))\n",
    "\n",
    "_, ax1 = plt.subplots(figsize=(7, 6))\n",
    "rating_counts.plot(kind='bar', width=0.7, color=colors)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Jumlah Ulasan')\n",
    "\n",
    "# Menambahkan nilai aktual di atas setiap bar\n",
    "for index, value in enumerate(rating_counts):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Menambahkan judul\n",
    "plt.title('Jumlah Ulasan Berdasarkan Rating')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISIS DESKRIPTIF LOKASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melihat banyak ulasan dari tiap rating\n",
    "\n",
    "df[\"Location\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Mengambil 10 lokasi terbanyak\n",
    "top_10_locations = df['Location'].value_counts().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Membuat daftar warna yang berbeda menggunakan palet warna matplotlib\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_10_locations)))\n",
    "\n",
    "_, ax1 = plt.subplots(figsize=(7, 6))\n",
    "top_10_locations.plot(kind='bar', width=0.7, color=colors)\n",
    "plt.xlabel('Lokasi')\n",
    "plt.ylabel('Jumlah Ulasan')\n",
    "\n",
    "# Menambahkan jumlah di atas setiap bar\n",
    "for index, value in enumerate(top_10_locations):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Menambahkan judul\n",
    "plt.title('Jumlah Ulasan Berdasarkan 10 Lokasi Terbanyak')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISIS DESKRIPTIF TAHUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan regex untuk mengekstrak tahun dari kolom Date\n",
    "df['Year'] = df['Date'].str.extract(r'(\\d{4})')\n",
    "\n",
    "# Menghitung jumlah ulasan untuk setiap tahun\n",
    "review_counts_by_year = df['Year'].value_counts().sort_index()\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(review_counts_by_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Menghitung jumlah ulasan untuk setiap tahun\n",
    "review_counts_by_year = df['Year'].value_counts().sort_index()\n",
    "\n",
    "# Membuat daftar warna yang berbeda menggunakan palet warna matplotlib\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(review_counts_by_year)))\n",
    "\n",
    "_, ax1 = plt.subplots(figsize=(7, 6))\n",
    "review_counts_by_year.plot(kind='bar', width=0.7, color=colors)\n",
    "plt.xlabel('Tahun')\n",
    "plt.ylabel('Jumlah Ulasan')\n",
    "\n",
    "# Menambahkan jumlah di atas setiap bar\n",
    "for index, value in enumerate(review_counts_by_year):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Menambahkan judul\n",
    "plt.title('Jumlah Ulasan Berdasarkan Tahun')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Tentukan path ke file Excel\n",
    "file_path = r'D:\\SAMUEL\\data mentah.xlsx'\n",
    "\n",
    "\n",
    "# Gunakan pandas untuk membaca file Excel\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Tampilkan isi DataFrame\n",
    "df.head()\n",
    "\n",
    "# menghapus baris jika ada data yang kosong\n",
    "\n",
    "df = df.dropna()\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE FOLDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASE FOLDING\n",
    "\n",
    "#mengubah semua huruf dalam dokumen menjadi huruf kecil (lower text)\n",
    "df['Reviews'] = df['Reviews'].str.lower()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('result case folding.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pembersihan teks dari tab, new life, back slice, mention, link, hastag, URL\n",
    "import string\n",
    "import re #regex library\n",
    "\n",
    "\n",
    "def remove_ulasan_spesial(text):\n",
    "  #menghapus tab new line dan back slice\n",
    "  text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\" \")\n",
    "  #menghapus non ASCII (emoticon, chinese word, etc)\n",
    "  text = text.encode('ascii','replace').decode('ascii')\n",
    "  #menghapus mention, link, hastag\n",
    "  text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "  #menghapus incomplete url\n",
    "  return text.replace(\"https://\",\" \").replace(\"https://\",\" \")\n",
    "\n",
    "\n",
    "df['Reviews'] = df['Reviews'].apply(remove_ulasan_spesial)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('result cleansing.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE NUMBER\n",
    "\n",
    "\n",
    "def remove_number(text) :\n",
    "    return re.sub(r\"\\d+\",\"\", text)\n",
    "\n",
    "\n",
    "df['Reviews'] = df['Reviews'].apply(remove_number)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('result remove number.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE PUNCTUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE PUNCTUATION\n",
    "\n",
    "\n",
    "#menghapus tanda baca\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "\n",
    "df['Reviews'] = df['Reviews'].apply(remove_punctuation)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('result remove punctuation.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE SINGLE CHARACTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Menghapus huruf tunggal\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n",
    "\n",
    "df['Reviews'] = df['Reviews'].apply(remove_single_char)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('hasil remove single char.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE SHORT WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE SHORT WORDS\n",
    "\n",
    "#menghapus kata-kata pendek (kurang dari 3 huruf)\n",
    "\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('hasil short words.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pemisahan teks menjadi potongan-potongan kata yang disebut token\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#NLTK word tokenize\n",
    "def word_tokenize_wrapper(text) :\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['Reviews'] = df['Reviews'].apply(word_tokenize_wrapper)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('hasil tokenizing.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPELL NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#menyeragamkan kata yang memiliki makna yang sama namun penulisannya berbeda\n",
    "\n",
    "# Tentukan path ke file Excel\n",
    "file_path_normalisasi = r'D:\\SAMUEL\\new_kamusalay.xlsx'\n",
    "\n",
    "\n",
    "normalizad_word = pd.read_excel(file_path_normalisasi)\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "   if row[0] not in normalizad_word_dict: \n",
    "     normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "def normalized_term(document):\n",
    "   return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term  in document]\n",
    "\n",
    "df[\"Reviews\"] = df[\"Reviews\"].apply(normalized_term)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Hasil normalisasi.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import stopwords from NLTK\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "# Menambahkan stopword tambahan\n",
    "list_stopwords.extend(['pas', 'ya', 'sih', 'deh', 'loh', 'oiya', 'nih', 'oke', 'ah', 'ok', 'lawang', 'sewu', 'semarang', 'belanda', 'nya'])\n",
    "\n",
    "# Membaca stopword dari file Excel\n",
    "txt_stopword = pd.read_excel('D:/SAMUEL/Stopwords Indonesia.xlsx', names=['stopwords'], header=None)\n",
    "\n",
    "# Mengambil kata-kata dari file Excel dan mengkonversi ke list\n",
    "additional_stopwords = txt_stopword['stopwords'][0].split(' ')\n",
    "\n",
    "# Menambahkan kata-kata yang mengandung \"di\" dari file Excel ke dalam list_stopwords\n",
    "list_stopwords.extend([word for word in additional_stopwords if 'di' in word])\n",
    "\n",
    "# Mengonversi list ke set\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "# Menghapus kata-kata yang mengandung \"di\"\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if all(stop not in word for stop in list_stopwords)]\n",
    "\n",
    "# Apply stopwords removal to the 'Reviews' column in your DataFrame\n",
    "df['Reviews'] = df['Reviews'].apply(stopwords_removal)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Hasil Filtering.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Hasil Prepocessing Indonesia.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ganti lokasi file Excel jika perlu\n",
    "file_path = 'D:/Semester 7/Tugas Akhir/Jupiter Samuel/Hasil Prepocessing Indonesia.xlsx'\n",
    "\n",
    "# Membaca data dari file Excel dan mengambil kolom 'Name', 'Rating', dan 'Reviews'\n",
    "df = pd.read_excel(file_path, usecols=['Name', 'Rating', 'Reviews'])\n",
    "\n",
    "# Mengganti nama kolom sesuai yang Anda inginkan\n",
    "df.columns = ['Name ', 'Rating', 'Reviews']\n",
    "\n",
    "# Menampilkan lima baris pertama dari dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Menggabungkan daftar token menjadi dokumen string tunggal\n",
    "\n",
    "import ast\n",
    "\n",
    "def join_text_list(texts):\n",
    "    texts = ast.literal_eval(texts)\n",
    "    return ' '.join([text for text in texts])\n",
    "\n",
    "df[\"Reviews\"] = df['Reviews'].apply(join_text_list)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Hasil Indonesia.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "review_text_list = df['Reviews'].tolist()\n",
    "\n",
    "reviews = review_text_list\n",
    "s = difflib.SequenceMatcher(None, reviews).ratio()\n",
    "print(\"ratio:\", s, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FREKUENSI TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import seaborn as sns\n",
    "\n",
    "def freq_words(x, terms = 30):\n",
    "    all_words = ' '.join([text for text in x])\n",
    "    all_words = all_words.split()\n",
    "\n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word': list(fdist.keys()), 'count':list(fdist.values())})\n",
    "\n",
    "    #memilih 25 term yang paling sering muncul\n",
    "    d = words_df.nlargest(columns=\"count\", n = terms)\n",
    "    plt.figure(figsize=(25,7))\n",
    "    ax = sns.barplot(data=d, x='word', y='count')\n",
    "    ax.set(ylabel = 'count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot term yang paling sering\n",
    "\n",
    "freq_words(df['Reviews'],25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEMBOBOTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pembobotan Kata \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cvec = CountVectorizer(stop_words='english', min_df=1, max_df=.5, ngram_range=(1,2))\n",
    "cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9045"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Menghitung semua n-gram yang ditemukan di semua dokumen\n",
    "\n",
    "from itertools import islice\n",
    "from nltk.corpus import stopwords\n",
    "global str\n",
    "\n",
    "cvec.fit(review_text_list)\n",
    "list(islice(cvec.vocabulary_.items(), 20))\n",
    "\n",
    "len(cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9045"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec_counts = CountVectorizer(stop_words='english', min_df=.0025, max_df=.5, ngram_range=(1,2))\n",
    "cvec.fit(review_text_list)\n",
    "len(cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_counts = cvec.transform(review_text_list)\n",
    "print('sparse matrix shape:', cvec_counts.shape)\n",
    "print('nonzero count:', cvec_counts.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * cvec_counts.nnz / (cvec_counts.shape[0] * cvec_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Menghitung frekuensi kemunculan term\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term' : cvec.get_feature_names_out(), 'occurences': occ})\n",
    "counts_df.sort_values(by='occurences', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sekarang kita memiliki jumlah term untuk setiap dokumen kita dapat menggunakan tfidf transformer untuk menghitung\n",
    "#bobot untuk setiap isitlah dalam setiap dokumen\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(cvec_counts)\n",
    "transformed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melihat 20 term teraatas dengan weight rata-rata tf-idf\n",
    "\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names_out(), 'weight' : weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PELABELAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membuat kamus kata\n",
    "\n",
    "word_dict = {}\n",
    "for i in range(0, len(df['Reviews'])):\n",
    "    sentence = df['Reviews'][i]\n",
    "    word_token = word_tokenize(sentence)\n",
    "    for j in word_token:\n",
    "        if j not in word_dict:\n",
    "            word_dict[j] = 1\n",
    "        else:\n",
    "            word_dict[j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len({k:v for (k,v) in word_dict.items() if v < 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Daftar kata-kata negasi\n",
    "negasi = ['bukan', 'tidak', 'ga', 'gk']\n",
    "\n",
    "# Impor lexicon dari file Excel\n",
    "lexicon = pd.read_excel('D:/SAMUEL/full_lexicon.xlsx')\n",
    "lexicon = lexicon.drop(lexicon[(lexicon['word'] == 'bukan')\n",
    "                               |(lexicon['word'] == 'tidak')\n",
    "                               |(lexicon['word'] == 'ga')\n",
    "                               |(lexicon['word']== 'gk')].index,axis=0)\n",
    "lexicon = lexicon.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "      <th>number_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>merekam</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ekstensif</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paripurna</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>detail</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pernik</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>belas</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>welas</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kabung</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rahayu</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  weight  number_of_words\n",
       "0        hai       3                1\n",
       "1    merekam       2                1\n",
       "2  ekstensif       3                1\n",
       "3  paripurna       1                1\n",
       "4     detail       2                1\n",
       "5     pernik       3                1\n",
       "6      belas       2                1\n",
       "7      welas       4                1\n",
       "8     kabung       1                1\n",
       "9     rahayu       4                1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_word = lexicon['word'].to_list()\n",
    "lexicon_num_words = lexicon['number_of_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10284"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memeriksa apakah ada kata dalam kamus yang tidak termasuk dalam lexicon\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "ns_words = []\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "for word in word_dict.keys():\n",
    "    kata_dasar = stemmer.stem(word)\n",
    "    if kata_dasar not in lexicon_word:\n",
    "        ns_words.append(word)\n",
    "len(ns_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melihat jenis kata,dimulai dengan beberapa kata yang memiliki banyak kemungkinan karena kemungkinan besar ini bukan tipe case\n",
    "\n",
    "len({k:v for (k,v) in word_dict.items() if ((k in ns_words)&(v>3))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_words_list = {k:v for (k,v) in word_dict.items() if ((k in ns_words)&(v>3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ternyata kata-kata yang tidak termasuk dalam lexion, adalah kata yang tidak memiliki arti sentimen\n",
    "\n",
    "sort_orders = sorted(ns_words_list.items(), key=lambda x: x[1], reverse=True)\n",
    "sort_orders = sort_orders[0:20]\n",
    "for i in sort_orders:\n",
    "\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentimen\n",
    "\n",
    "lexicon['number_of_words'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'pakerti' in word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'budi baik' in lexicon_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Menghitung sentimen kata dengan menghitungnya menjadi lexicon\n",
    "sencol = []\n",
    "senrow = np.array([])\n",
    "nsen = 0\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "sentiment_list = []\n",
    "\n",
    "# Fungsi untuk menuliskan sentimen kata jika sudah ditemukan\n",
    "def found_word(ind, words, word, sen, sencol, sentiment, add):\n",
    "    # Jika sudah termasuk dalam bag of words matrix, maka naikkan (tambahkan) saja nilai\n",
    "    if word in sencol:\n",
    "        sen[sencol.index(word)] += 1\n",
    "    else:\n",
    "        # Jika tidak, maka tambahkan kata baru\n",
    "        sencol.append(word)\n",
    "        sen.append(1)\n",
    "        add += 1\n",
    "    # Jika ada kata negasi sebelumnya, sentimen adalah negasi sentimennya\n",
    "    if (words[ind - 1] in negasi):\n",
    "        sentiment += -lexicon['weight'][lexicon_word.index(word)]\n",
    "    else:\n",
    "        sentiment += lexicon['weight'][lexicon_word.index(word)]\n",
    "\n",
    "    return sen, sencol, sentiment, add\n",
    "\n",
    "# Memeriksa setiap kata, jika muncul dalam lexicon dan kemudian menghitung sentimennya\n",
    "for i in range(len(df)):\n",
    "    nsen = senrow.shape[0]\n",
    "    words = word_tokenize(df['Reviews'][i])\n",
    "    sentiment = 0\n",
    "    add = 0\n",
    "    prev = [0 for ii in range(len(words))]\n",
    "    n_words = len(words)\n",
    "    if len(sencol) > 0:\n",
    "        sen = [0 for j in range(len(sencol))]\n",
    "    else:\n",
    "        sen = []\n",
    "\n",
    "    for word in words:\n",
    "        ind = words.index(word)\n",
    "        # Periksa apakah mereka termasuk dalam lexicon\n",
    "        if word in lexicon_word:\n",
    "            sen, sencol, sentiment, add = found_word(ind, words, word, sen, sencol, sentiment, add)\n",
    "        else:\n",
    "            # Jika tidak, maka periksa kata dasarnya\n",
    "            kata_dasar = stemmer.stem(word)\n",
    "            if kata_dasar in lexicon_word:\n",
    "                sen, sencol, sentiment, add = found_word(ind, words, kata_dasar, sen, sencol, sentiment, add)\n",
    "            # Jika masih negatif, coba cocokkan kombinasi kata dengan kata yang berdekatan\n",
    "            elif n_words > 1:\n",
    "                if ind - 1 > -1:\n",
    "                    back_1 = words[ind - 1] + ' ' + word\n",
    "                    if back_1 in lexicon_word:\n",
    "                        sen, sencol, sentiment, add = found_word(ind, words, back_1, sen, sencol, sentiment, add)\n",
    "                    elif ind - 2 > -1:\n",
    "                        back_2 = words[ind - 2] + ' ' + back_1\n",
    "                        if back_2 in lexicon_word:\n",
    "                            sen, sencol, sentiment, add = found_word(ind, words, back_2, sen, sencol, sentiment, add)\n",
    "\n",
    "    # Jika ada kata baru yang ditemukan, maka perluas matriks\n",
    "    if add > 0:\n",
    "        if i > 0:\n",
    "            if (nsen == 0):\n",
    "                senrow = np.zeros([i, add], dtype=int)\n",
    "            elif (i != nsen):\n",
    "                padding_h = np.zeros([nsen, add], dtype=int)\n",
    "                senrow = np.hstack((senrow, padding_h))\n",
    "                padding_v = np.zeros([(i - nsen), senrow.shape[1]], dtype=int)\n",
    "                senrow = np.vstack((senrow, padding_v))\n",
    "            else:\n",
    "                padding = np.zeros([nsen, add], dtype=int)\n",
    "                senrow = np.hstack((senrow, padding))\n",
    "            senrow = np.vstack((senrow, sen))\n",
    "        if i == 0:\n",
    "            senrow = np.array(sen).reshape(1, len(sen))\n",
    "    # Jika tidak ada maka perbaharui saja matriks lama\n",
    "    elif (nsen > 0):\n",
    "        senrow = np.vstack((senrow, sen))\n",
    "\n",
    "    sentiment_list.append(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membangun kerangka data yang berisi kumpulan kata dan sentimen yang telah dihitung sebelumnya\n",
    "\n",
    "sencol.append('sentiment')\n",
    "sentiment_array = np.array(sentiment_list).reshape(senrow.shape[0],1)\n",
    "sentiment_data = np.hstack((senrow,sentiment_array))\n",
    "df_sen = pd.DataFrame(sentiment_data,columns = sencol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sen.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melihat sentimen dari data orginal\n",
    "\n",
    "res_df = pd.DataFrame([])\n",
    "res_df['Rating'] = df['Rating'].copy()\n",
    "res_df['Name'] = df['Name '].copy()\n",
    "res_df['Reviews'] = df['Reviews'].copy()\n",
    "res_df['sentiment'] = df_sen['sentiment'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengubah penilaian menjadi label \n",
    "#pelabelan data,data akan berlabel negatif(0) jika nilai compound <0,0 dan akan bernilai positif(1) jika nilai compound >=0,0\n",
    "\n",
    "label = []\n",
    "for index, row in res_df.iterrows():\n",
    "    if row['sentiment'] >= 0:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(0)\n",
    "res_df['label'] = label\n",
    "res_df = res_df.drop(columns=['sentiment'])\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melihat banyak ulasan dari tiap label\n",
    "\n",
    "res_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METODE KLASIFIKASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klasifikasiLawangSewu = res_df\n",
    "klasifikasiLawangSewu = res_df.drop(columns=['Rating','Name'])\n",
    "klasifikasiLawangSewu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA TRAINING DAN DATA TESTING\n",
    "\n",
    "#data training dan data testing\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(klasifikasiLawangSewu['Reviews'],klasifikasiLawangSewu['label'],test_size=0.2, random_state=8)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(klasifikasiLawangSewu['Reviews'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Mechine\n",
    "\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale')\n",
    "SVM.fit(Train_X_Tfidf, Train_Y)\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"SVM Accuracy Score ->\",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAIVE BAYES\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "NB = naive_bayes.MultinomialNB()\n",
    "NB.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_NB = NB.predict(Test_X_Tfidf)\n",
    "\n",
    "print('NB Accuracy Score ->',accuracy_score(predictions_NB, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  # Perbaiki impor ini\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(Test_Y, predictions_SVM)\n",
    "class_label = ['negative', 'positive']\n",
    "test = pd.DataFrame(conf_mat, index=class_label, columns=class_label)\n",
    "sns.heatmap(test, annot=True, fmt=\"d\")\n",
    "\n",
    "plt.title(\"Confusion Matrix for Test Data\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")  # Perbaiki kesalahan penulisan \"ylabel\"\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALISASI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LawangSewuNeg = klasifikasiLawangSewu.loc[klasifikasiLawangSewu['label']==0]\n",
    "LawangSewuNeg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "neg_text_list = LawangSewuNeg['Reviews'].tolist()\n",
    "reviews_neg = neg_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(len(english_stopwords))\n",
    "text_neg = str(neg_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEMBUAT POTONGAN KATA\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens_neg = word_tokenize(text_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "negatice = [w for w in tokens_neg if not w in stop_words]\n",
    "extend = 'a'\n",
    "negative = [w for w in negative if not w in extend]\n",
    "\n",
    "negative = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in negative]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "table = str.maketrans('','',string.punctuation)\n",
    "strippedneg = [w.translate(table) for w in negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words(strippedneg,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "import matplotlib.pyplot as plt  # Tambahkan impor ini\n",
    "\n",
    "all_text_negative = ' '.join(str(word) for word in strippedneg)\n",
    "wordcloud = WordCloud(max_font_size=260, max_words=50, width=1000, height=1000, mode=\"RGBA\", background_color='white').generate(all_text_negative)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SENTIMEN POSITIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LawangSewuPos = klasifikasiLawangSewu.loc[klasifikasiLawangSewu['label']==1]\n",
    "LawangSewuPos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "pos_text_list = LawangSewuPos['Reviews'].tolist()\n",
    "\n",
    "reviews_pos = pos_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(len(english_stopwords))\n",
    "text_pos = str(pos_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membuat potongan kata(token)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens_pos = word_tokenize(text_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "positive = [w for w in tokens_pos if not w in stop_words]\n",
    "positive = [w for w in positive if not w in extend]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "strippedpos = [w.translate(table) for w in positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memplot kata-kata yang paling sering muncul\n",
    "\n",
    "freq_words(strippedpos,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORDCLOUD POSITIF\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "\n",
    "all_text_positive = ' '.join(str(word) for word in strippedpos)\n",
    "wordcloud = WordCloud(max_font_size=260, max_words=50, width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_positive)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.margins(x=0,y=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
